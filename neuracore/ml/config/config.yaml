defaults:
  - _self_
  - algorithm: null
  - override hydra/launcher: basic
  

# Core training parameters
seed: 42
epochs: 100
output_prediction_horizon: 100
validation_split: 0.2
logging_frequency: 50
keep_last_n_checkpoints: 5 
device: null  # e.g., "cuda:0", "mps", "cpu"

# Batch size (can be "auto" for automatic tuning or an integer)
batch_size: "auto"

num_train_workers: 2
num_val_workers: 1

# Maximum number of workers to use for prefetching recordings
max_prefetch_workers: 4

# Dataset synchronization
dataset_name: null
dataset_id: null
frequency: 10

# You can either specify input_data_types/output_data_types or
# input_robot_data_spec/output_robot_data_spec
input_data_types:
  - "JOINT_POSITIONS"
  - "RGB_IMAGES"

output_data_types:
  - "JOINT_TARGET_POSITIONS"

# Dict[str, Dict[DataType, List[str]], e.g., {"robot-id-1": {"JOINT_POSITIONS": ["joint_1", "joint_2", ...]}}
# You can also pass in an empty dict {} to use all available data for all robots
input_robot_data_spec: null
output_robot_data_spec: null

# Paths and IDs (for cloud vs local training)
algorithm_id: null    # For cloud training
training_id: null     # For cloud training
org_id: null          # For cloud training
run_name: null        # Optional name for the training run. If not provided, a three-word random name will be generated.
local_output_dir: ${oc.env:HOME}/.neuracore/training/runs/${now:%Y-%m-%d_%H-%M-%S}

# Custom algorithm parameters (used when algorithm_id is provided)
# These will be passed directly to the algorithm constructor
algorithm_params: null

# Resume training
resume_checkpoint_path: null  # Path to checkpoint to resume from

batch_size_autotuning_num_workers: 4 # number of workers to use for batch size autotuning

# Hydra configuration
hydra:
  run:
    dir: ${local_output_dir}
  sweep:
    dir: ${local_output_dir}
    subdir: ${hydra:job.num}